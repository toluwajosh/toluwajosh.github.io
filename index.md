---
layout: default
---

![Avatar](media/avatar_square_small.jpg)

Joshua Owoyemi is a PhD, Autonomous Driving Perception and Deep Learning Engineer. This website is a collection of his works and portfolio.

Download his [full CV](media/joshua_owoyemi_full_cv.pdf){:target="_blank"}.

## Profile Links

| [LinkedIn](https://www.linkedin.com/in/joshua-owoyemi/) | [GitHub](https://github.com/toluwajosh) | [Twitter](https://twitter.com/toluwajosh) | [Facebook](https://www.facebook.com/toluwajosh) | [Instagram](https://www.instagram.com/toluwajosh/) |

## Current Roles

**Machine learning and Software Engineer** at [ZMP Inc](https://www.zmp.co.jp/) Tokyo, Japan; Developing deep learning-based software solutions for Autonomous Car's perception, scene understanding, and behaviour control.

**Online Content Creator, Tutor and Consultant**; Delivering [Videos](https://www.youtube.com/channel/UCdNgkkleUb_npdGLgzzh_2g) and [Podcasts](https://anchor.fm/joshua-owoyemi) focusing on Technology and Education.

<!-- See past roles.. -->

## Selected Publications

**Joshua Owoyemi**, Naoya Chiba, Koichi Hashimoto (2019). Discriminative Recognition of Point Cloud Gesture Classes Through One-Shot Learning. 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), Dali, China, 2019, pp. 2304-2309. ([IEEE Xplore](https://ieeexplore.ieee.org/document/8961778))

**Joshua Owoyemi**, Koichi Hashimoto (2018). Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data. 2018 IEEE International Conference on Robotics and Automation (ICRA) May 21-25, 2018, Brisbane, Australia. pp.5929-5934, 2018. ([https://arxiv.org/abs/1804.08859v1](https://arxiv.org/abs/1804.08859v1))

Cherdsak Kingkan, **Joshua Owoyemi**, Koichi Hashimoto (2018). “Point Attention Network for Gesture Recognition Using Point Cloud Data”, 29th British Machine Vision Conference, September 3-6, Newcastle, England, pp.118.1-118.13, 2018. *1st and 2nd Author have equal contribution*. ([pdf](http://bmvc2018.org/contents/papers/0427.pdf))

**Joshua Owoyemi**, Koichi Hashimoto (2017). Learning Human Motion Intention with 3D Convolutional Neural Network. 2017 IEEE International Conference on Mechatronics and Automation August 6-9, Takamatsu, Japan. pp.1810-1815, 2017. ([IEEE Xplore](https://ieeexplore.ieee.org/document/8016092))

## Other Projects

* Robot Control and Manipulation for Liquid Pouring ([Video](https://youtu.be/oCoAv5VNHO0)): This involved developing a point cloud gesture-based robot operation and a manipulation strategy for a robot arm to pour liquids without spilling. Simulation in with ROS and Gazebo to validate pouring strategy.

* Upper Body Point Cloud Gestures Dataset ([UBPG](https://github.com/toluwajosh/ubpg)): This is a point cloud based gesture dataset captured by the Kinect camera using the Point Cloud Library. It contains 9 classes of gesture with 1 class for No Gesture. This dataset capture classes of dynamic gestures through a 3 dimensional data representation posing challenges of patter recognition in the spatial and temporal space. It is free to be used for research purposes.

<!-- See more.. -->

## Presentations and Talks

* Getting Started: Hardware and Software ([Video](https://youtu.be/6hkSMOQQBrE)). A Presentation at the TEDx Tohoku University AI Salon Sendai, Japan.

* Fast Motion Inference Learning with One-Shot Learning from Class Embedding. The 5th Case Western Reserve University - Tohoku University Joint Workshop, Sendai, Japan, August 2nd-3rd, 2018.

## Awards

Japan Ministry of Education, Culture, Sports, Science and Technology Scholarship (2015-2019)
